{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAS BDAI Individual Innovation Project: Tennis Match Predictor\n",
    "\n",
    "## Table of Contents \n",
    "1. [Introduction](#introduction)\n",
    "2. [Preliminary steps](#preliminary-steps)\n",
    "3. [ ](# )\n",
    "4. [ ](# )\n",
    "5. [ ](# )\n",
    "6. [ ](# )\n",
    "7. [ ](# )\n",
    "8. [ ](# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "### Tennis Match Predictor: GAImeSetMatch\n",
    "\n",
    "### Goal of this project\n",
    "\n",
    "### Steps to implement\n",
    "1. Load and explore the data\n",
    "2. Data processing and cleaning\n",
    "3. Feature Engineering\n",
    "    - Surface win %\n",
    "    - Tournament level win %\n",
    "    - Head-to-head\n",
    "    - Recent form\n",
    "4. Data Analysis\n",
    "5. Prediction\n",
    "\n",
    "\n",
    "![.png](img/project/image.png)\n",
    "\n",
    "Image source: [something](https://example.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary steps <a name=\"preliminary-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to the interpreter (OPTIONAL - skip if using Google Colab; modify if using local dev environment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/jean/Documents/dev/cas-project/venv_proj/bin/python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependencies\n",
    "We need to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set static parameters\n",
    "Here we set some parameters which won't be changed. This allows for more easy handling and viewing of the data being explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, set some static parameters and options (used later too for loading other files)\n",
    "\n",
    "# directory containing the .csv files\n",
    "DIRNAME = 'data'\n",
    "\n",
    "# set options for pandas viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions and datasets\n",
    "These will help us later with common tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample data 5 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small sample dataframe (5 matches) for misc usage\n",
    "sample_matches_df = pd.DataFrame(data = {\n",
    "    'tourney_id': ['2000-301', '2000-301', '2000-301', '2000-301', '2000-301'],\n",
    "    'tourney_name': ['Auckland', 'Auckland', 'Auckland', 'Auckland', 'Auckland'],\n",
    "    'surface': ['Hard', 'Hard', 'Hard', 'Hard', 'Hard'],\n",
    "    'draw_size': [32, 32, 32, 32, 32],\n",
    "    'tourney_level': ['A', 'A', 'A', 'A', 'A'],\n",
    "    'tourney_date': [20000110, 20000110, 20000110, 20000110, 20000110],\n",
    "    'match_num': [1, 2, 3, 4, 5],\n",
    "    'winner_id': [103163, 102607, 103252, 103507, 102103],\n",
    "    'winner_seed': [1.0, None, None, 7.0, None],\n",
    "    'winner_entry': [None, 'Q', None, None, 'Q'],\n",
    "    'winner_name': ['Tommy Haas', 'Juan Balcells', 'Alberto Martin', 'Juan Carlos Ferrero', 'Michael Sell'],\n",
    "    'winner_hand': ['R', 'R', 'R', 'R', 'R'],\n",
    "    'winner_ht': [188.0, 190.0, 175.0, 183.0, 180.0],\n",
    "    'winner_ioc': ['GER', 'ESP', 'ESP', 'ESP', 'USA'],\n",
    "    'winner_age': [21.7, 24.5, 21.3, 19.9, 27.3],\n",
    "    'loser_id': [101543, 102644, 102238, 103819, 102765],\n",
    "    'loser_seed': [None, None, None, None, 4.0],\n",
    "    'loser_entry': [None, None, None, None, None],\n",
    "    'loser_name': ['Jeff Tarango', 'Franco Squillari', 'Alberto Berasategui', 'Roger Federer', 'Nicolas Escude'],\n",
    "    'loser_hand': ['L', 'L', 'L', 'L', 'L'],\n",
    "    'loser_ht': [180.0, 183.0, 173.0, 185.0, 185.0],\n",
    "    'loser_ioc': ['USA', 'ARG', 'ESP', 'SUI', 'FRA'],\n",
    "    'loser_age': [31.1, 24.3, 26.5, 18.4, 23.7],\n",
    "    'score': ['7-5 4-6 7-5', '7-5 7-5', '6-3 6-1', '6-4 6-4', '0-6 7-6(7) 6-1'],\n",
    "    'best_of': [3, 3, 3, 3, 3],\n",
    "    'round': ['R32', 'R32', 'R32', 'R32', 'R32'],\n",
    "    'minutes': [108.0, 85.0, 56.0, 68.0, 115.0],\n",
    "    'w_ace': [18.0, 5.0, 0.0, 5.0, 1.0],\n",
    "    'w_df': [4.0, 3.0, 0.0, 1.0, 2.0],\n",
    "    'w_svpt': [96.0, 76.0, 55.0, 53.0, 98.0],\n",
    "    'w_1stIn': [49.0, 52.0, 35.0, 28.0, 66.0],\n",
    "    'w_1stWon': [39.0, 39.0, 25.0, 26.0, 39.0],\n",
    "    'w_2ndWon': [28.0, 13.0, 12.0, 15.0, 14.0],\n",
    "    'w_SvGms': [17.0, 12.0, 8.0, 10.0, 13.0],\n",
    "    'w_bpSaved': [3.0, 5.0, 1.0, 0.0, 6.0],\n",
    "    'w_bpFaced': [5.0, 6.0, 1.0, 0.0, 8.0],\n",
    "    'l_ace': [7.0, 10.0, 6.0, 11.0, 8.0],\n",
    "    'l_df': [8.0, 7.0, 6.0, 2.0, 8.0],\n",
    "    'l_svpt': [106.0, 74.0, 56.0, 70.0, 92.0],\n",
    "    'l_1stIn': [55.0, 32.0, 33.0, 43.0, 46.0],\n",
    "    'l_1stWon': [39.0, 25.0, 20.0, 29.0, 34.0],\n",
    "    'l_2ndWon': [29.0, 18.0, 7.0, 14.0, 18.0],\n",
    "    'l_SvGms': [17.0, 12.0, 8.0, 10.0, 12.0],\n",
    "    'l_bpSaved': [4.0, 3.0, 7.0, 6.0, 5.0],\n",
    "    'l_bpFaced': [7.0, 6.0, 11.0, 8.0, 9.0],\n",
    "    'winner_rank': [11.0, 211.0, 48.0, 45.0, 167.0],\n",
    "    'winner_rank_points': [1612.0, 157.0, 726.0, 768.0, 219.0],\n",
    "    'loser_rank': [63.0, 49.0, 59.0, 61.0, 34.0],\n",
    "    'loser_rank_points': [595.0, 723.0, 649.0, 616.0, 873.0]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore the data\n",
    "This section loads the data available in .csv files from the aforementioned source, explores the data and then cleans it for ease of use and data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load matches\n",
    "Data is available in the form of results of ATP matches. For simplicity reasons, focus only on matches since the year 2000*. Each year is stored in one file using naming convention atp_matches_yyyy.csv.\n",
    "\n",
    "*The reasoning behind this: since the year 2000, there have been factors that have influenced the outcomes of the modern form of the sport. For me, these are:\n",
    "1. Racquet technology: Since the 1980s, rackets are made mainly out of graphite. Reference: [Link](https://www.pledgesports.org/2019/08/evolution-of-tennis-rackets/)\n",
    "2. String technology: In the late 1990s, polyester strings were introduced, which revolutionised the sport. Reference: [Link](https://scientificinquirer.com/2021/08/30/string-theory-the-synthetic-revolution-that-changed-tennis-forever/)\n",
    "3. Surfaces: in 2009, the ATP discontinued use of carpet court use in all its tournaments. Reference: [Link](https://racketsportsworld.com/tennis-not-played-carpet-courts/#When_was_Carpet_Discontinued_from_Use_in_Tennis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of matches (since the year 2000 ) files to load\n",
    "atp_match_files = [f'{DIRNAME}/atp_matches_{year}.csv' for year in range(2000, 2024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to store all matches\n",
    "matches_df = pd.DataFrame()\n",
    "\n",
    "# loop through the list of match files, read them and append the data to the combined DataFrame\n",
    "for filen in atp_match_files:\n",
    "    matches_df = pd.concat([matches_df, pd.read_csv(filen, index_col=None)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the matches data\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of number of features, instances, empty values and data types \n",
    "matches_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alll features starting with \"w_\" or \"l_\" indicate in-game metrics, which is out of scope for this project. So we will remove them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of instances and features: \" + str(matches_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zeros\n",
    "Here we check for zeros in the matches mframe, in order to decide what to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all features for zero's\n",
    "zero_count_per_feature= matches_df.apply(lambda col: (col == 0).sum())\n",
    "zero_count_per_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the matches with 0 or less minutes\n",
    "matches_lessthan_0mins = matches_df.loc[matches_df['minutes']<=0]\n",
    "matches_lessthan_0mins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches lasting 0 minutes are all W/O (\"Walkovers\"), meaning that one player did not contest the match due to injury, illness, etc. These instances should not be used for predicting matches, as they don't measure a player's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score contains text\n",
    "Sometimes the score feature contains text, like \"RET\" (match retirement), in addition to the previously observation about W/O. If we want to calculate the number of games played, we should remove this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_score_text = matches_df[matches_df['score'].str.contains('[a-zA-Z]')]\n",
    "matches_score_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN or empty values\n",
    "Here we check for NaN or empty values in the matches mframe, in order to decide what to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all features for empty values\n",
    "empty_count_per_feature= matches_df.isnull().sum()\n",
    "empty_count_per_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the features starting with \"w_\" or \"l_\", there are 8 features in the matches dataset which have empty values, and indication whether this will be used for prediction or not:\n",
    "1. minutes - not used\n",
    "2. seed - not used\n",
    "3. entry - not used\n",
    "4. hand - not used\n",
    "5. ht (height) - not used\n",
    "6. age - not used\n",
    "7. rank - used\n",
    "8. rank_points - not used\n",
    "\n",
    "Of these 8 features, only 1 will be used: rank. Let's explore a few of these matches with an empty rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the matches with empty rank\n",
    "matches_empty_rank = matches_df.loc[matches_df['winner_rank'].isnull() | matches_df['loser_rank'].isnull()]\n",
    "matches_empty_rank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches with players having no (empty) rank could be because they are new, or have been inactive due to injury and hence lost their ranking before returning. We can try and look up their last valid ranking in the rankings file later. \n",
    "\n",
    "Next, do the values for rank make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_with_rank = matches_df.loc[~matches_df['winner_rank'].isnull() & ~matches_df['loser_rank'].isnull()]\n",
    "\n",
    "# Plot 2 histograms for distribution of values for \"rank\"\n",
    "# Create subplots for the histograms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot the first histogram for winner_rank\n",
    "ax1.hist(matches_with_rank['winner_rank'], bins=20, color='blue', alpha=0.7)\n",
    "ax1.set_title('Distribution of winner rank')\n",
    "ax1.set_xlabel('Rank')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# Plot the second histogram for loser_rank\n",
    "ax2.hist(matches_with_rank['loser_rank'], bins=20, color='green', alpha=0.7)\n",
    "ax2.set_title('Distribution of loser rank')\n",
    "ax2.set_xlabel('Rank')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "# Display the histograms\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that most matches are won by players ranked in the top 100 (~60'000), which makes sense. Also, there are no outlier values like rank=5'000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could matches with empty minutes be due to the tourney_level?\n",
    "# print(matches_empty_minutes['tourney_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tournament start dates\n",
    "It would be interesting to see on which weekdays tournaments start. Becuase later, we want to link the rankings data with the matches data, so a common day of the week  would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date from tourney_date \n",
    "matches_tournament_starts = matches_df.copy()\n",
    "matches_tournament_starts['tourney_date_dt'] = pd.to_datetime(matches_df['tourney_date'], format='%Y%m%d')\n",
    "\n",
    "# create a column representing the day of the week\n",
    "matches_tournament_starts['tourney_date_dt_day_name'] = matches_tournament_starts['tourney_date_dt'].dt.day_name()\n",
    "\n",
    "# day of week frequency for matches and rankingsday of week frequency for matches and rankings\n",
    "matches_tournament_starts['tourney_date_dt_day_name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, ca. **81%** of the matches started on a Monday. This is a strong case to say that for simplicity, we set all matches to start at the beginning of the week which would be Monday. But before doing this, let's see which matches don't start on a Monday and group by tournament type, then display the results using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the matches_tournament_starts by 'tourney_level' and 'tourney_date_dt_day_name' and count the occurrences\n",
    "matches_tournament_starts = matches_tournament_starts.groupby(['tourney_level', 'tourney_date_dt_day_name']).size().unstack().fillna(0)\n",
    "\n",
    "# Create a stacked bar chart\n",
    "matches_tournament_starts.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Tournament Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Tournament Start Weekday by Tournament Type')\n",
    "\n",
    "# Display the legend\n",
    "plt.legend(title='Tournament Start Weekday', loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [matches_data_dictionary.txt](data/matches_data_dictionary.txt):\n",
    "- 'G' = Grand Slams\n",
    "- 'M' = Masters 1000s\n",
    "- 'A' = other tour-level events\n",
    "- 'C' = Challengers\n",
    "- 'S' = Satellites/ITFs\n",
    "- 'F' = Tour finals and other season-ending events\n",
    "- 'D' = Davis Cup \n",
    "\n",
    "Most tournaments start on a Monday, with a notable exception: Davis Cup, which are run over weekends and start on a Friday. \n",
    "**Decision**: For better linking with rankings, we've decided that we will set all tournaments' start dates to the Monday which precedes it. For example, if its Friday yyyy-mm-dd, then a supplemental date feature will be provided for its preceding Monday yyyy-mm-dd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction benchmark for matches\n",
    "In order to evaluate the prediction accuracy of our model, we need a benchmark to compare when predicting the results of matches. One simple benchmark would be to assume that the higher (i.e. closer to 1) ranked player will always win. This \"higher-ranked player win ratio\" can easily be calculated using the features available in the original dataset.\n",
    "We know that some rankings are empty, so we will just substitute a number higher than the max. ranking (which is 2101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a prediction benchmark, empty ranking means no ranking, so replace with a arbitrary high value\n",
    "matches_wins_by_ranking_df = matches_df.copy()\n",
    "matches_wins_by_ranking_df[['winner_rank','loser_rank']] = matches_wins_by_ranking_df[['winner_rank','loser_rank']].fillna(value=10000)\n",
    "\n",
    "# add a new feature which is the result of checking whether the winner was ranked higher (i.e. closer to 1) than the loser\n",
    "matches_wins_by_ranking_df['winning_player_ranked'] = matches_wins_by_ranking_df.apply(lambda x: \"higher\" if x['winner_rank'] < x['loser_rank'] else \"lower\", axis=1)\n",
    "matches_wins_by_ranking_df['winning_player_ranked'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that for our dataset, the higher ranked player won **65.6%** of all the matches. This will be our benchmark for evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load rankings\n",
    "Data is also available in the form of ranking of ATP players. It may be required to supplement the missing data for current rankings in the matches dataset, for example, a player doesn't have a ranking at the time of playing a match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the rankings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of rankings (since the year 2000 ) files to load\n",
    "atp_rankings_files = [f'{DIRNAME}/atp_rankings_{year}.csv' for year in ['00s','10s', '20s', 'current']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to store all rankings\n",
    "rankings_df = pd.DataFrame()\n",
    "\n",
    "# loop through the list of rankings files, read them and append the data to the combined DataFrame\n",
    "for filen in atp_rankings_files:\n",
    "    rankings_df = pd.concat([rankings_df, pd.read_csv(filen, index_col=None)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the rankings data\n",
    "rankings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of number of features, instances, empty values and data types \n",
    "rankings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks on the data (min values, max values, etc.)\n",
    "rankings_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, the min and max values for the rankings make sense. Also, the ranking_date makes sense. Finally, there are no missing values, so no data cleaning is required on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of instances and features: \" + str(rankings_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking dates\n",
    "Similar to which weekdays tournaments start, let's look at the days on which the rankings get updated. Becuase later, we want to link the rankings data with the matches data, so a common day of the week  would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date from ranking_date \n",
    "ranking_update = rankings_df.copy()\n",
    "ranking_update['ranking_date_dt'] = pd.to_datetime(rankings_df['ranking_date'], format='%Y%m%d')\n",
    "\n",
    "# create a column representing the day of the week\n",
    "ranking_update['ranking_date_dt_day_name'] = ranking_update['ranking_date_dt'].dt.day_name()\n",
    "\n",
    "# day of week frequency for ranking\n",
    "ranking_update['ranking_date_dt_day_name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All (100%)** of the rankings are updated on a Monday. Therefore, we are aligned with the idea to set all tournament start dates to a Monday.\n",
    "\n",
    "Below is a final view of the loaded data for rankings, with the new column for the datetime formatted `ranking_date_dt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "print(rankings_df.info())\n",
    "\n",
    "# preview data\n",
    "rankings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing and cleaning\n",
    "\n",
    "### Scope of processing and cleaning\n",
    "1. Replace the matches' winner and loser columns\n",
    "2. Clean the date features and make them consistent\n",
    "3. Ensure the matches are sorted as needed\n",
    "4. Remove matches with result as W/O\n",
    "5. Players without rankings: \n",
    "- seasoned players  (they had a long layoff due to injury, etc.). keep match and lookup ranking from earlier. Apply penalty of 10 ranking places for each week they were absent.\n",
    "- if they played less than 10 matches (cumulative) - remove match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the processing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with a copy of the original loaded dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_processed_df = matches_df.copy()\n",
    "matches_processed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_processed_df = rankings_df.copy()\n",
    "rankings_processed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hide winner and loser from columns names\n",
    "Replace columns starting with 'winner_' and 'loser_' with 'player_1_' and 'player_2_' for the required features. As we want to be able to predict who will be the winner and the loser in each match, we remove the 'winner_' and 'loser_' columns for each match, and instead replace it with player_1_ and player_2 according to which the ranking of the players. \n",
    "\n",
    "The features starting with 'w_' and 'l_' are measures recorded during the match and will not be used in the model for predicting the outcome, so we remove these features.\n",
    "We will add a column at the end of the dataframe, which will serve as our y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_winner_loser(input_df):\n",
    "    # List of required features to be replaced with prefixes player_1 and player_2\n",
    "    features = ['id', 'seed', 'entry', 'name', 'hand', 'ht', 'ioc', 'age', 'rank', 'rank_points']\n",
    "    \n",
    "    # Copy the input DataFrame to a new one\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # Add player_1_name and player_2_name columns based on higher rank\n",
    "    df['player_1_name'] = np.where((df['winner_rank'].fillna(float('inf')) <= df['loser_rank'].fillna(float('inf'))),\n",
    "                                   df['winner_name'],\n",
    "                                   df['loser_name']\n",
    "                                   )\n",
    "    df['player_2_name'] = np.where((df['winner_rank'].fillna(float('inf')) > df['loser_rank'].fillna(float('inf'))),\n",
    "                                   df['winner_name'],\n",
    "                                   df['loser_name']\n",
    "                                   )\n",
    "\n",
    "    # Transfer the values from 'winner_' and 'loser_' features to 'player_1_' and 'player_2_' features, according to who was the winner & loser\n",
    "    for feat in features:\n",
    "        player_1_feature = np.where(df['player_1_name'] == df['winner_name'],\n",
    "                                    df['winner_' + feat],\n",
    "                                    df['loser_' + feat]\n",
    "                                    )\n",
    "        player_2_feature = np.where(df['player_2_name'] == df['winner_name'],\n",
    "                                    df['winner_' + feat],\n",
    "                                    df['loser_' + feat]\n",
    "                                    )\n",
    "        df['player_1_' + feat] = player_1_feature\n",
    "        df['player_2_' + feat] = player_2_feature   \n",
    "\n",
    "          \n",
    "    # Add a winner column\n",
    "    df['winner'] = df.apply(lambda row: 'player_1' if row['winner_name'] == row['player_1_name'] else 'player_2', axis=1)\n",
    "\n",
    "    # Remove columns starting with 'winner_' and 'loser_' (they have been replaced by player_1_ and player_2_)\n",
    "    df = df.loc[:, ~df.columns.str.startswith('winner_') & ~df.columns.str.startswith('loser_')]\n",
    "\n",
    "    # Remove columns starting with 'w_' and 'l_' (not needed for predicting_)\n",
    "    df = df.loc[:, ~df.columns.str.startswith('w_') & ~df.columns.str.startswith('l_')]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function hide_winner_loser with a sample dataset of 5 instances. Observe the renamed features, from \"winnner_\" and \"loser_\" to \"player_1\" and \" player_2\", and the new feature called \"winner\" (our y variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = hide_winner_loser(sample_matches_df)\n",
    "output_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[['tourney_id'\n",
    "           , 'player_1_name', 'player_1_rank'\n",
    "           , 'player_2_name', 'player_2_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the winner and loser columns with player_1 and player_2 for the matches dataset\n",
    "matches_processed_df= hide_winner_loser(matches_processed_df)\n",
    "matches_processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean and consistent date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column for datetime datatype version of the date columns\n",
    "matches_processed_df['tourney_date_dt'] = pd.to_datetime(matches_processed_df['tourney_date'], format='%Y%m%d')\n",
    "rankings_processed_df['ranking_date_dt'] = pd.to_datetime(rankings_processed_df['ranking_date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure the matches are sorted as needed\n",
    "This is crucial as we are calculating cumulative measures (e.g. count of prior matches) to base a prediction on. It's not required for the rankings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort matches by tourney_date, tourney_id and match_num, and reset the index as the old one is not required anymore.\n",
    "matches_processed_df = matches_processed_df.sort_values(['tourney_date', 'tourney_id', 'match_num'], ascending=True)\n",
    "matches_processed_df = matches_processed_df.reset_index(drop=True) \n",
    "matches_processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove matches with result as W/O\n",
    "W/O stands for \"Walkover\". Matches resulting in W/O should not be considered, so remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove matches resulting in a W/O\n",
    "matches_processed_df = matches_processed_df[matches_processed_df['score'] != 'W/O']\n",
    "len(matches_processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down from 71'213 to 70'910 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove matches with 0 or less minutes\n",
    "Not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove matches with 0 or less minutes\n",
    "# matches_df = matches_df.loc[matches_df['minutes']<0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add feature for matches dataset that all tournaments start dates are shown as a Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature for tournaments not starting on a Monday, with its value being the preceding Monday\n",
    "matches_processed_df['tourney_date_dt'] = pd.to_datetime(matches_processed_df['tourney_date'], format='%Y%m%d')\n",
    "matches_processed_df['tourney_date_dt_preceding_monday'] = matches_processed_df['tourney_date_dt'].apply(lambda x: x - pd.DateOffset(days=x.weekday()) if x.weekday() != 0 else x)\n",
    "\n",
    "# verify that this feature's date values are all on a Monday\n",
    "matches_processed_df['tourney_date_dt_preceding_monday'].dt.day_name().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check examples of these new column values compared to its original\n",
    "matches_processed_df[matches_processed_df['tourney_date_dt'].dt.day_name() != 'Monday'][['tourney_date_dt', 'tourney_date_dt_preceding_monday']].groupby(['tourney_date_dt', 'tourney_date_dt_preceding_monday']).size().reset_index(name='count').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 28 Jan. 2000 was a Friday, and 24 Jan. 2000 was the preceding Monday\n",
    "- 4 Feb. 2000 was a Friday, and 31 Jan. 2000 was the preceding Monday\n",
    "- 17 Mar. 2000 was a Friday, and 13 Mar. 2000 was the preceding Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches_processed_df.to_csv(\"matches_processed_df.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process matches with new players having no ranking \n",
    "Remove matches where 1 opponent has so far played less than 10 completed matches. \n",
    "Notes: \n",
    "- Don't remove matches in the year 2000, as our players could have played 10 matches prior to the year 2000, and our cumulative count features need a year to get working.\n",
    "- W/O matches don't count, but retirements do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example of players where 1 opponent has so far played < 10 completed matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process matches with seasoned players having no ranking\n",
    "As explained before, there are matches with seasoned (experienced on the ATP Tour) players having no (empty) rank possibly because they have been inactive due to injury and hence lost their ranking before returning. If they are not new players, we can try and look up their last valid ranking in the rankings file. A recent example is Kevin Anderson, who was inactive for a period due to retiring in May 2022 and then announcing his comeback in July 2023* \n",
    "\n",
    "*Source: [Wikipedia \"Kevin_Anderson (tennis)\", accessed Oct. 2023](https://en.wikipedia.org/wiki/Kevin_Anderson_(tennis))\n",
    "\n",
    "We will: \n",
    "1. for a particular match, find the latest available historical ranking in the rankings dataset for the player in the matches dataset\n",
    "2. add 10 to the ranking for each week where the player was inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of player previously having a ranking but later no ranking\n",
    "matches_processed_ka_df = matches_processed_df[(matches_processed_df['player_1_name'] == 'Kevin Anderson') \n",
    "                                                | (matches_processed_df['player_2_name'] == 'Kevin Anderson')].tail()\n",
    "matches_processed_ka_df[['tourney_date_dt', 'player_1_id', 'player_2_id', 'player_1_name', 'player_2_name', 'player_1_rank', 'player_2_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define static parameter\n",
    "rank_penalty_per_week_inactivity = 10\n",
    "\n",
    "# define a function to look up historic rankings for players having no ranking in a particular match\n",
    "def impute_missing_rankings(m, r):\n",
    "    last_rankings = {}  # Dictionary to store the last available player_rank for each player_id\n",
    "\n",
    "    for i, row in m.iterrows():\n",
    "        if pd.isna(row['player_1_rank']):\n",
    "            week = row['tourney_date_dt'] - dt.timedelta(days=row['tourney_date_dt'].weekday())\n",
    "            p1_id = row['player_1_id']\n",
    "\n",
    "            # Find the last available ranking date prior to the tourney_date_dt\n",
    "            last_ranking_date = r[(r['player'] == p1_id) & (r['ranking_date_dt'] < week)]['ranking_date_dt'].max()\n",
    "\n",
    "            if last_ranking_date:\n",
    "                last_ranking_row = r[(r['player'] == p1_id) & (r['ranking_date_dt'] == last_ranking_date)]\n",
    "                if not last_ranking_row.empty:\n",
    "                    last_rank = last_ranking_row['rank'].values[0]\n",
    "                    weeks_difference = (week - last_ranking_date).days // 7\n",
    "                    imputed_rank = last_rank + weeks_difference * rank_penalty_per_week_inactivity\n",
    "                    if imputed_rank > 3333: # Don't over-penalize\n",
    "                        m.at[i, 'player_1_rank'] = 3333   # Set a default value   \n",
    "                    else: \n",
    "                        m.at[i, 'player_1_rank'] = imputed_rank\n",
    "                        last_rankings[p1_id] = imputed_rank\n",
    "                else:\n",
    "                    m.at[i, 'player_1_rank'] = 3333  # Set a default value\n",
    "            else:\n",
    "                m.at[i, 'player_1_rank'] = 3333  # Set a default value\n",
    "\n",
    "        if pd.isna(row['player_2_rank']):\n",
    "            week = row['tourney_date_dt'] - dt.timedelta(days=row['tourney_date_dt'].weekday())\n",
    "            p2_id = row['player_2_id']\n",
    "\n",
    "            # Find the last available ranking date prior to the tourney_date_dt\n",
    "            last_ranking_date = r[(r['player'] == p2_id) & (r['ranking_date_dt'] < week)]['ranking_date_dt'].max()\n",
    "\n",
    "            if last_ranking_date:\n",
    "                last_ranking_row = r[(r['player'] == p2_id) & (r['ranking_date_dt'] == last_ranking_date)]\n",
    "                if not last_ranking_row.empty:\n",
    "                    last_rank = last_ranking_row['rank'].values[0]\n",
    "                    weeks_difference = (week - last_ranking_date).days // 7\n",
    "                    imputed_rank = last_rank + weeks_difference * rank_penalty_per_week_inactivity\n",
    "                    if imputed_rank > 3333: # Don't over-penalize\n",
    "                        m.at[i, 'player_2_rank'] = 3333   # Set a default value   \n",
    "                    else:\n",
    "                        m.at[i, 'player_2_rank'] = imputed_rank\n",
    "                        last_rankings[p2_id] = imputed_rank\n",
    "                else: \n",
    "                    m.at[i, 'player_2_rank'] = 3333  # Set a default value\n",
    "            else:\n",
    "                m.at[i, 'player_2_rank'] = 3333  # Set a default value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function using the example of Kevin Anderson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function using only reduced dataset: Kevin Anderson\n",
    "impute_missing_rankings(matches_processed_ka_df, rankings_processed_df)\n",
    "matches_processed_ka_df[['tourney_date_dt', 'player_1_name', 'player_2_name', 'player_1_rank', 'player_2_rank','round', 'winner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_processed_df[(rankings_processed_df['ranking_date'].between(20220501, 20230801)) & (rankings_processed_df['player'] == 104731)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_ka_inactive = (dt.date(2023,7,17) - dt.date(2022,5,23)).days  // 7 # no. or weeks inactivity\n",
    "weeks_ka_inactive * 10 # rank place penalty of 10 per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty of roughly 600 places for a 60 week period of inactivity reflects roughly the output of the function impute_missing_rankings.\n",
    "\n",
    "Finally, we apply the function to our full dataset, and do a small check to verify that no null values exist anymore for these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_missing_rankings(matches_processed_df, rankings_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_processed_df['player_1_rank'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_processed_df['player_2_rank'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_processed_df[['player_1_rank', 'player_2_rank']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new copy of the dataframe, for starting the feature engineering\n",
    "matches_features_df = matches_processed_df.copy().reset_index(drop=True)\n",
    "matches_features_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add feature for cumulative number of games played so far in a tournament - OPTIONAL and WIP\n",
    "It would be interesting if the cumulative number of games played so far in a tournament could be used to predict the next result of a match, indicating either fatigue or dominance (won in straight sets). So far this feature is optional for our prediction model.\n",
    "\n",
    "*Note: this function does not yet work 100%. It doesn't yet calculate the `player_x_tourney_cum_games_count` correctly in the case where a player can appear as player_1 or player_2 in the same tournament.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calc_game_counts: to calculate game counts for each match, uses vectorization instead of row iteration for performance reasons\n",
    "'''\n",
    "def calc_game_counts(m):\n",
    "    # Initialize columns to store game counts\n",
    "    m['player_1_match_games_count'] = 0\n",
    "    m['player_2_match_games_count'] = 0\n",
    "    m['player_1_tourney_cum_games_count'] = 0\n",
    "    m['player_2_tourney_cum_games_count'] = 0\n",
    "    \n",
    "    # Regular expression to match scores\n",
    "    score_pattern = r'(\\d+)-(\\d+)(?:\\(\\d+\\))?'\n",
    "    \n",
    "    # Extract individual scores using regular expression and convert to numeric values\n",
    "    scores = m['score'].str.extractall(score_pattern).astype(int)\n",
    "    m[['player_1_games', 'player_2_games']] = scores.groupby(level=0).sum()\n",
    "    \n",
    "    # Determine the winner and adjust game counts accordingly\n",
    "    winner_mask = m['winner'] == 'player_1'\n",
    "    m.loc[winner_mask, 'player_1_match_games_count'] = m.loc[winner_mask, 'player_1_games']\n",
    "    m.loc[~winner_mask, 'player_1_match_games_count'] = m.loc[~winner_mask, 'player_2_games']\n",
    "    \n",
    "    m['player_2_match_games_count'] = m['player_1_games'] + m['player_2_games'] - m['player_1_match_games_count']\n",
    "    \n",
    "    # Calculate cumulative game counts using groupby and cumsum without resetting\n",
    "    m['player_1_tourney_cum_games_count'] = m.groupby(['tourney_id', 'player_1_id'])['player_1_match_games_count'].cumsum() - m['player_1_match_games_count']\n",
    "    m['player_2_tourney_cum_games_count'] = m.groupby(['tourney_id', 'player_2_id'])['player_2_match_games_count'].cumsum() - m['player_2_match_games_count']\n",
    "    \n",
    "    # Set the initial cumulative game counts to 0 for the first matches of each player\n",
    "    m.loc[m.groupby(['tourney_id', 'player_1_id'])['player_1_match_games_count'].cumcount() == 0, 'player_1_tourney_cum_games_count'] = 0\n",
    "    m.loc[m.groupby(['tourney_id', 'player_2_id'])['player_2_match_games_count'].cumcount() == 0, 'player_2_tourney_cum_games_count'] = 0\n",
    "    \n",
    "    return m\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function calc_game_counts on the matches dataset\n",
    "'''\n",
    "matches_processed_df = calc_game_counts(matches_processed_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test by exporting to .csv\n",
    "'''matches_processed_df[matches_processed_df['tourney_id'].str.contains('2000-451|2000-301')][['tourney_id','match_num', 'player_1_id','player_2_id',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t   'player_1_name','player_2_name', 'round' ,'score', 'winner', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'player_1_match_games_count', 'player_2_match_games_count',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'player_1_tourney_cum_games_count', 'player_2_tourney_cum_games_count']].to_csv(\"matches_processed.csv\", sep=',', header=True, index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the output of the new columns for the first 31 rows (1 tournament)\n",
    "'''\n",
    "matches_processed_df[['tourney_id','match_num', 'player_1_id','player_2_id',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t   'player_1_name','player_2_name', 'round' ,'score', 'winner', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'player_1_match_games_count', 'player_2_match_games_count',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'player_1_tourney_cum_games_count', 'player_2_tourney_cum_games_count']].head(31)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add feature for ranking difference\n",
    "This feature may help our model more easily assess the how the ranking plays a factor in determining the winner of the match. It simply calculates the weight of the difference between player_2_rank and player_1_rank, by using a normalized difference. The normalized difference is expressed as a number between 0 and 1. In that case, the closer the ranking between player 1 and player 2, the higher the number will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max. possible rank difference\n",
    "max_possible_rank_difference = max(matches_features_df['player_2_rank'] - matches_features_df['player_1_rank'])\n",
    "\n",
    "# calculate normalized rank difference\n",
    "matches_features_df['ranking_difference'] = 1 - ((matches_features_df['player_2_rank'] - matches_features_df['player_1_rank']) / max_possible_rank_difference)\n",
    "\n",
    "# preview the result for the last 5 observations of the dataset\n",
    "matches_features_df[['tourney_date_dt', 'player_1_name', 'player_1_rank','player_2_name', 'player_2_rank', 'ranking_difference']].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the result for the first 5 observations of the dataset\n",
    "matches_features_df[['tourney_date_dt', 'player_1_name', 'player_1_rank','player_2_name', 'player_2_rank', 'ranking_difference']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add feature for cumulative matches played count, and win percentages per surface and tourney level for player 1 and player 2\n",
    "This cumulative matches played count and win percentages per surface and tourney level are important features for our prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surface win %\n",
    "This feature will show a player's success so far on a particular tennis court surface. There will be a number expressed as a percentage which will reflect the number of wins divided by the total matches on a surface, prior to that match taking place.\n",
    "First, what are the different surfaces being played on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the different surfaces played on since 2000?\n",
    "matches_features_df['surface'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the different surfaces played on in the top 10 tournaments in 2000?\n",
    "print(matches_features_df[matches_features_df['surface'] == 'Hard']['tourney_name'].value_counts().head(10))\n",
    "print(matches_features_df[matches_features_df['surface'] == 'Clay']['tourney_name'].value_counts().head(10))\n",
    "print(matches_features_df[matches_features_df['surface'] == 'Grass']['tourney_name'].value_counts().head(10))\n",
    "print(matches_features_df[matches_features_df['surface'] == 'Carpet']['tourney_name'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test dataset for all 4 surface types, and preview the columns and sample rows relevant for calculation\n",
    "matches_4surfaces = matches_features_df[(matches_features_df['tourney_name'].isin(['Auckland', 'Barcelona', 'Halle', 'Basel']))\n",
    "\t\t\t\t\t& (matches_features_df['tourney_date'] < 20010000)][['tourney_name', 'tourney_date_dt', 'match_num', 'surface', 'tourney_level'\n",
    "                                                                                , 'player_1_id', 'player_2_id', 'player_1_name', 'player_2_name'\n",
    "                                                                                , 'winner'\n",
    "\t\t\t\t\t]]\n",
    "matches_4surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tournament level win %\n",
    "This feature will show a player's success so far on a particular type (level) of tournament. There will be a number expressed as a percentage which will reflect the number of wins divided by the total matches on that level, prior to that match taking place.\n",
    "First, what are the different tournament level being played?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the different tournament levels played since 2000?\n",
    "matches_features_df['tourney_level'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate win %s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cum_match_counts_and_pct (df):\n",
    "\n",
    "    # Initialize dictionaries to keep track of cumulative match counts and wins for each player and surface\n",
    "    player_cumulative_counts = {}\n",
    "    player_surface_cumulative_counts = {}\n",
    "    player_surface_cumulative_wins = {}\n",
    "    player_tourney_level_cumulative_counts = {}\n",
    "    player_tourney_level_cumulative_wins = {}\n",
    "\n",
    "    # Lists to store the cumulative match counts for each row\n",
    "    player_1_cumulative_counts_list = []\n",
    "    player_2_cumulative_counts_list = []\n",
    "    player_1_surface_cumulative_counts_list = []\n",
    "    player_2_surface_cumulative_counts_list = []\n",
    "    player_1_surface_cumulative_wins_list = []\n",
    "    player_2_surface_cumulative_wins_list = []\n",
    "    player_1_tourney_level_cumulative_counts_list = []\n",
    "    player_2_tourney_level_cumulative_counts_list = []\n",
    "    player_1_tourney_level_cumulative_wins_list = []\n",
    "    player_2_tourney_level_cumulative_wins_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        player_1_id = row['player_1_id']\n",
    "        player_2_id = row['player_2_id']\n",
    "        surface = row['surface']\n",
    "        tourney_level = row['tourney_level']\n",
    "\n",
    "        # Get the cumulative match counts so far for each player\n",
    "        player_1_cumulative_count = player_cumulative_counts.get(player_1_id, 0)\n",
    "        player_2_cumulative_count = player_cumulative_counts.get(player_2_id, 0)\n",
    "\n",
    "        # Get the cumulative match counts and wins on the current surface for each player\n",
    "        player_1_surface_cumulative_count = player_surface_cumulative_counts.get((player_1_id, surface), 0)\n",
    "        player_2_surface_cumulative_count = player_surface_cumulative_counts.get((player_2_id, surface), 0)\n",
    "        player_1_surface_cumulative_wins = player_surface_cumulative_wins.get((player_1_id, surface), 0)\n",
    "        player_2_surface_cumulative_wins = player_surface_cumulative_wins.get((player_2_id, surface), 0)\n",
    "\n",
    "        # Get the cumulative match counts and wins on the current tourney level for each player\n",
    "        player_1_tourney_level_cumulative_count = player_tourney_level_cumulative_counts.get((player_1_id, tourney_level), 0)\n",
    "        player_2_tourney_level_cumulative_count = player_tourney_level_cumulative_counts.get((player_2_id, tourney_level), 0)\n",
    "        player_1_tourney_level_cumulative_wins = player_tourney_level_cumulative_wins.get((player_1_id, tourney_level), 0)\n",
    "        player_2_tourney_level_cumulative_wins = player_tourney_level_cumulative_wins.get((player_2_id, tourney_level), 0)\n",
    "\n",
    "        # Update the cumulative match counts and wins for each player, surface and tourney level in the current players' lists\n",
    "        player_1_cumulative_counts_list.append(player_1_cumulative_count)\n",
    "        player_2_cumulative_counts_list.append(player_2_cumulative_count)\n",
    "        player_1_surface_cumulative_counts_list.append(player_1_surface_cumulative_count)\n",
    "        player_2_surface_cumulative_counts_list.append(player_2_surface_cumulative_count)\n",
    "        player_1_tourney_level_cumulative_counts_list.append(player_1_tourney_level_cumulative_count)\n",
    "        player_2_tourney_level_cumulative_counts_list.append(player_2_tourney_level_cumulative_count)\n",
    "\n",
    "        # Calculate and update the cumulative match won percentage on the current surface for each player\n",
    "        player_1_surface_cumulative_wins_percentage = (\n",
    "            player_1_surface_cumulative_wins / player_1_surface_cumulative_count\n",
    "        ) if player_1_surface_cumulative_count > 0 else 0.0\n",
    "        player_2_surface_cumulative_wins_percentage = (\n",
    "            player_2_surface_cumulative_wins / player_2_surface_cumulative_count\n",
    "        ) if player_2_surface_cumulative_count > 0 else 0.0\n",
    "\n",
    "        player_1_surface_cumulative_wins_list.append(player_1_surface_cumulative_wins_percentage)\n",
    "        player_2_surface_cumulative_wins_list.append(player_2_surface_cumulative_wins_percentage)\n",
    "\n",
    "        # Calculate and update the cumulative match won percentage on the current tourney level for each player\n",
    "        player_1_tourney_level_cumulative_wins_percentage = (\n",
    "            player_1_tourney_level_cumulative_wins / player_1_tourney_level_cumulative_count\n",
    "        ) if player_1_tourney_level_cumulative_count > 0 else 0.0\n",
    "        player_2_tourney_level_cumulative_wins_percentage = (\n",
    "            player_2_tourney_level_cumulative_wins / player_2_tourney_level_cumulative_count\n",
    "        ) if player_2_tourney_level_cumulative_count > 0 else 0.0\n",
    "\n",
    "        player_1_tourney_level_cumulative_wins_list.append(player_1_tourney_level_cumulative_wins_percentage)\n",
    "        player_2_tourney_level_cumulative_wins_list.append(player_2_tourney_level_cumulative_wins_percentage)\n",
    "\n",
    "        # Increment the cumulative match counts and wins for each player and surface in the dictionaries\n",
    "        player_cumulative_counts[player_1_id] = player_1_cumulative_count + 1\n",
    "        player_cumulative_counts[player_2_id] = player_2_cumulative_count + 1\n",
    "        player_surface_cumulative_counts[(player_1_id, surface)] = player_1_surface_cumulative_count + 1\n",
    "        player_surface_cumulative_counts[(player_2_id, surface)] = player_2_surface_cumulative_count + 1\n",
    "        player_tourney_level_cumulative_counts[(player_1_id, tourney_level)] = player_1_tourney_level_cumulative_count + 1\n",
    "        player_tourney_level_cumulative_counts[(player_2_id, tourney_level)] = player_2_tourney_level_cumulative_count + 1\n",
    "\n",
    "        # Increment the cumulative match wins on the current surface for the winner\n",
    "        if row['winner'] == 'player_1':\n",
    "            player_surface_cumulative_wins[(player_1_id, surface)] = player_1_surface_cumulative_wins + 1\n",
    "        else:\n",
    "            player_surface_cumulative_wins[(player_2_id, surface)] = player_2_surface_cumulative_wins + 1\n",
    "\n",
    "        # Increment the cumulative match wins on the current tourney level for the winner\n",
    "        if row['winner'] == 'player_1':\n",
    "            player_tourney_level_cumulative_wins[(player_1_id, tourney_level)] = player_1_tourney_level_cumulative_wins + 1\n",
    "        else:\n",
    "            player_tourney_level_cumulative_wins[(player_2_id, tourney_level)] = player_2_tourney_level_cumulative_wins + 1\n",
    "\n",
    "    # Add the cumulative match count and surface- and tourney level-related columns to the input dataset\n",
    "    df['player_1_cum_match_count'] = player_1_cumulative_counts_list\n",
    "    df['player_2_cum_match_count'] = player_2_cumulative_counts_list\n",
    "    df['player_1_surface_cum_match_count'] = player_1_surface_cumulative_counts_list\n",
    "    df['player_2_surface_cum_match_count'] = player_2_surface_cumulative_counts_list\n",
    "    df['player_1_surface_cum_win_percentage'] = player_1_surface_cumulative_wins_list\n",
    "    df['player_2_surface_cum_win_percentage'] = player_2_surface_cumulative_wins_list\n",
    "    df['player_1_tourney_level_cum_match_count'] = player_1_tourney_level_cumulative_counts_list\n",
    "    df['player_2_tourney_level_cum_match_count'] = player_2_tourney_level_cumulative_counts_list\n",
    "    df['player_1_tourney_level_cum_win_percentage'] = player_1_tourney_level_cumulative_wins_list\n",
    "    df['player_2_tourney_level_cum_win_percentage'] = player_2_tourney_level_cumulative_wins_list\n",
    "\n",
    "    # Add win percentage difference columns for surface- and tourney level\n",
    "    df['surface_win_pct_difference'] = df['player_1_surface_cum_win_percentage'] - df['player_2_surface_cum_win_percentage']\n",
    "    df['tourney_level_win_pct_difference'] = df['player_1_tourney_level_cum_win_percentage'] - df['player_2_tourney_level_cum_win_percentage']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the data\n",
    "matches_features_df = calc_cum_match_counts_and_pct(matches_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the function on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test surface win pct and tourney level win pct for one player\n",
    "matches_features_df[(matches_features_df['player_1_name'] == 'Roger Federer') \n",
    "                     |\n",
    "                     (matches_features_df['player_2_name'] == 'Roger Federer')][['tourney_name', 'tourney_date_dt', 'match_num', 'surface', 'tourney_level'\n",
    "                                                                                , 'player_1_name', 'player_2_name'\n",
    "                                                                                , 'winner'\n",
    "                                                                                 , 'player_1_surface_cum_win_percentage','player_2_surface_cum_win_percentage'\n",
    "                                                                                 , 'player_1_tourney_level_cum_win_percentage','player_2_tourney_level_cum_win_percentage'\n",
    "                                                                                 , 'surface_win_pct_difference', 'tourney_level_win_pct_difference'\n",
    "                                                                                 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test surface win pct and tourney level win pct for another player\n",
    "matches_features_df[(matches_features_df['player_1_name'] == 'Thomas Enqvist') \n",
    "                    | (matches_features_df['player_2_name'] == 'Thomas Enqvist')][['tourney_name', 'tourney_date_dt', 'match_num', 'surface', 'tourney_level'\n",
    "                                                                                , 'player_1_name', 'player_2_name'\n",
    "                                                                                , 'winner'\n",
    "                                                                                , 'player_1_surface_cum_win_percentage','player_2_surface_cum_win_percentage'\n",
    "                                                                                , 'player_1_tourney_level_cum_win_percentage','player_2_tourney_level_cum_win_percentage'\n",
    "                                                                                , 'surface_win_pct_difference', 'tourney_level_win_pct_difference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for 4 tournaments, each on different surface\n",
    "matches_4surfaces_calc = calc_cum_match_counts_and_pct(matches_4surfaces)\n",
    "matches_4surfaces_calc[['tourney_name', 'tourney_date_dt', 'match_num', 'surface'\n",
    "                                                                                , 'player_1_name', 'player_2_name'\n",
    "                                                                                , 'winner'\n",
    "                                                                                 , 'player_1_surface_cum_win_percentage','player_2_surface_cum_win_percentage'\n",
    "                                                                                 , 'surface_win_pct_difference', 'tourney_level_win_pct_difference']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test dataset for all 5 tournament levels, and preview the columns and sample rows relevant for calculation\n",
    "matches_5levels = matches_features_df[(matches_features_df['tourney_name'].isin(['Auckland', 'Davis Cup QLS R1: GER vs SUI', 'Tour Finals', 'Australian Open', 'Indian Wells Masters',]))\n",
    "\t\t\t\t\t& (matches_features_df['tourney_date'] < 20010000)][['tourney_name', 'tourney_date_dt', 'match_num', 'tourney_level'\n",
    "                                                                                , 'player_1_id', 'player_2_id', 'player_1_name', 'player_2_name'\n",
    "                                                                                , 'winner'\n",
    "                                                                                , 'player_1_tourney_level_cum_win_percentage','player_2_tourney_level_cum_win_percentage'\n",
    "                                                                                , 'tourney_level_win_pct_difference'\n",
    "\t\t\t\t\t                                                    ]]\n",
    "matches_5levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head-to-head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the head-to-head record for two players, and expresses the result as a percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_h2h_win_pct(df):\n",
    "\n",
    "\n",
    "    # Create a dictionary to store cumulative wins and matches for each pair of players\n",
    "    h2h_stats = {}\n",
    "\n",
    "    # Initialize new columns\n",
    "    df['player_1_h2h_win_pct'] = 0.0\n",
    "    df['player_2_h2h_win_pct'] = 0.0\n",
    "\n",
    "    # Calculate head-to-head win percentage\n",
    "    for index, row in df.iterrows():\n",
    "        player_1_id = row['player_1_id']\n",
    "        player_2_id = row['player_2_id']\n",
    "        winner = row['winner']\n",
    "\n",
    "        # Create a unique key for the pair of players\n",
    "        player_pair_key = tuple(sorted([player_1_id, player_2_id]))\n",
    "\n",
    "        # Update head-to-head stats for the player pair\n",
    "        h2h_stats[player_pair_key] = h2h_stats.get(player_pair_key, {'ppk_1_wins': 0, 'ppk_2_wins': 0, 'matches': 0}) # ppk stands for \"player pair key\"\n",
    "\n",
    "        # Calculate and update head-to-head win percentages\n",
    "        if h2h_stats[player_pair_key]['matches'] == 0:\n",
    "            # At the first match, both win percentages are set to 0\n",
    "            df.at[index, 'player_1_h2h_win_pct'] = 0.0\n",
    "            df.at[index, 'player_2_h2h_win_pct'] = 0.0\n",
    "        else:\n",
    "            # For subsequent matches, calculate based on the previous match\n",
    "            if player_1_id == player_pair_key[0]: \n",
    "                player_1_win_pct = h2h_stats[player_pair_key]['ppk_1_wins'] / h2h_stats[player_pair_key]['matches']\n",
    "            else: \n",
    "                player_1_win_pct = h2h_stats[player_pair_key]['ppk_2_wins'] / h2h_stats[player_pair_key]['matches']\n",
    "            df.at[index, 'player_1_h2h_win_pct'] = player_1_win_pct\n",
    "            df.at[index, 'player_2_h2h_win_pct'] = 1.0 - player_1_win_pct\n",
    "\n",
    "        # Update head-to-head stats for the player pair after the match\n",
    "        h2h_stats[player_pair_key]['matches'] += 1\n",
    "        if ((winner == 'player_1') & (player_1_id == player_pair_key[0]) | (winner == 'player_2') & (player_2_id == player_pair_key[0])):\n",
    "            h2h_stats[player_pair_key]['ppk_1_wins'] += 1\n",
    "        else:\n",
    "            h2h_stats[player_pair_key]['ppk_2_wins'] += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the data\n",
    "matches_features_df = calc_h2h_win_pct(matches_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a h2h difference\n",
    "matches_features_df['h2h_win_pct_difference'] = matches_features_df['player_1_h2h_win_pct'] - matches_features_df['player_2_h2h_win_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test dataset for 2 players' head-to-head matches, and preview the columns and sample rows relevant for calculation\n",
    "\n",
    "matches_2players = matches_features_df[((matches_features_df['player_1_name'] == 'Andrey Rublev') & (matches_features_df['player_2_name'] == 'Jannik Sinner')) | \n",
    "                 ((matches_features_df['player_1_name'] == 'Jannik Sinner') & (matches_features_df['player_2_name'] == 'Andrey Rublev'))][['tourney_name', 'tourney_date_dt', 'match_num', 'tourney_level', 'round'\n",
    "                                                                                                                                        , 'player_1_id', 'player_2_id'\n",
    "                                                                                                                                        , 'player_1_name', 'player_2_name'\n",
    "                                                                                                                                        , 'player_1_rank', 'player_2_rank'\n",
    "                                                                                                                                        , 'winner'\n",
    "                                                                                                                                        , 'player_1_h2h_win_pct','player_2_h2h_win_pct'\n",
    "                                                                                                                                        , 'h2h_win_pct_difference'\n",
    "                                                                                                                                        ]]\n",
    "matches_2players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basics first before moving to the prediction models: what features are we left with after data processing and feature engineering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_features_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have engineered so far these features for assisting our prediction model:\n",
    "- ranking_difference\n",
    "- player_1_surface_cum_win_percentage     \n",
    "- player_2_surface_cum_win_percentage\n",
    "- surface_win_pct_difference\n",
    "- player_1_tourney_level_cum_win_percentage\n",
    "- player_2_tourney_level_cum_win_percentage\n",
    "- tourney_level_win_pct_difference\n",
    "- player_1_h2h_win_pct \n",
    "- player_2_h2h_win_pct\n",
    "- h2h_win_pct_difference\n",
    "\n",
    "The majority of the other features in our dataset are probably not needed,  # , 'player_1_tourney_level_cum_win_percentage'\n",
    "                                                        # , 'player_2_tourney_level_cum_win_percentage'and when applying encoding, we'll end up with a lot of additional useless features. \n",
    "So as a final step, we remove unused features from out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_features_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data for the year 2000, so the majority of features with values = 0 is removed\n",
    "matches_features_df = matches_features_df[matches_features_df['tourney_date_dt'].dt.year > 2000]\n",
    "matches_features_df.groupby(matches_features_df['tourney_date_dt'].dt.year)['tourney_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_features_trimmed_df = matches_features_df.drop(columns=[\n",
    "                                                        'tourney_id'\n",
    "                                                        , 'tourney_name'\n",
    "                                                        , 'surface'\n",
    "                                                        , 'draw_size'\n",
    "                                                        , 'tourney_level'\n",
    "                                                        , 'tourney_date'\n",
    "                                                        , 'match_num'\n",
    "                                                        , 'score'\n",
    "                                                        , 'best_of'\n",
    "                                                        , 'round'\n",
    "                                                        , 'minutes'\n",
    "                                                        , 'player_1_name'\n",
    "                                                        , 'player_2_name'\n",
    "                                                        , 'player_1_id'\n",
    "                                                        , 'player_2_id'\n",
    "                                                        , 'player_1_seed'\n",
    "                                                        , 'player_2_seed'\n",
    "                                                        , 'player_1_entry'\n",
    "                                                        , 'player_2_entry'\n",
    "                                                        , 'player_1_hand'\n",
    "                                                        , 'player_2_hand'\n",
    "                                                        , 'player_1_ht'\n",
    "                                                        , 'player_2_ht'\n",
    "                                                        , 'player_1_ioc'\n",
    "                                                        , 'player_2_ioc'\n",
    "                                                        , 'player_1_age'\n",
    "                                                        , 'player_2_age'\n",
    "                                                        , 'player_1_rank'\n",
    "                                                        , 'player_2_rank'\n",
    "                                                        , 'player_1_rank_points'\n",
    "                                                        , 'player_2_rank_points'\n",
    "                                                        # , 'winner'\n",
    "                                                        , 'tourney_date_dt'\n",
    "                                                        , 'tourney_date_dt_preceding_monday'\n",
    "                                                        #, 'ranking_difference'\n",
    "                                                        , 'player_1_cum_match_count'\n",
    "                                                        , 'player_2_cum_match_count'\n",
    "                                                        , 'player_1_surface_cum_match_count'\n",
    "                                                        , 'player_2_surface_cum_match_count'\n",
    "                                                        , 'player_1_surface_cum_win_percentage'\n",
    "                                                        , 'player_2_surface_cum_win_percentage'\n",
    "                                                        # , 'tourney_level_win_pct_difference'\n",
    "                                                        , 'player_1_tourney_level_cum_match_count'\n",
    "                                                        , 'player_2_tourney_level_cum_match_count'\n",
    "                                                        , 'player_1_tourney_level_cum_win_percentage'\n",
    "                                                        , 'player_2_tourney_level_cum_win_percentage'\n",
    "                                                        #, 'tourney_level_win_pct_difference'\n",
    "                                                        , 'player_1_h2h_win_pct'\n",
    "                                                        , 'player_2_h2h_win_pct'\n",
    "                                                        # , 'h2h_win_pct_difference'\n",
    "                                                        ], axis=1)\n",
    "matches_features_trimmed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new copy of the dataframe, prior to starting the prediction\n",
    "matches_pred_df = matches_features_trimmed_df.copy().reset_index(drop=True)\n",
    "matches_pred_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical data\n",
    "train = pd.get_dummies(matches_pred_df, drop_first=True)\n",
    "train.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type of label from bool to int64\n",
    "train['winner_player_2'] = train['winner_player_2'].astype('int64')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Determine correlations between variables in the dataset\n",
    "corr = train.corr()\n",
    "\n",
    "# Plot the correlations\n",
    "ax = sns.heatmap(corr, annot=True, cmap='coolwarm', cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title(\"Correlations of variables in the full dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- there are no really strong correlation (>0.7 or <-0.7) surface and tourney level have a strong correlation of 0.68 to 0.7\n",
    "- there is a medium strong correlation between surface_win_pct_difference and tourney_level_win_pct_difference\n",
    "\n",
    "Due to the low number of variables, and their insignificant correlation, its probably not beneficial to remove any variables in order to resolve the problem of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Now that the initial datasets have been used, we cleaup the pandas dataframes which are not required anymore to mitigate excessive memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import gc\n",
    "\n",
    "del matches_2players\n",
    "del matches_4surfaces\n",
    "del matches_5levels\n",
    "del matches_4surfaces_calc\n",
    "del matches_df\n",
    "del matches_empty_rank\n",
    "del matches_features_df\n",
    "del matches_features_trimmed_df\n",
    "del matches_lessthan_0mins\n",
    "del matches_pred_df\n",
    "del matches_processed_df\n",
    "del matches_processed_ka_df\n",
    "del matches_score_text\n",
    "del matches_tournament_starts\n",
    "del matches_wins_by_ranking_df\n",
    "del matches_with_rank\n",
    "\n",
    "# Invoke garbage collector immediately\n",
    "gc.collect()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we apply several different prediction models to determine which one gives us the most accurate results for predicting the outcome of a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual vs predicted plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_vs_predicted_plot(y_true, y_pred):\n",
    "  min_value=np.array([y_true.min(), y_pred.min()]).min()\n",
    "  max_value= min=np.array([y_true.max(), y_pred.max()]).max()\n",
    "  fig = plt.figure()\n",
    "  ax = fig.gca()\n",
    "  ax.scatter(y_true,y_pred, color=\"blue\")\n",
    "  ax.plot([min_value,max_value], [min_value, max_value], lw=4, color=\"green\")\n",
    "  ax.set_xlabel('Actual')\n",
    "  ax.set_ylabel('Predicted')\n",
    "  plt.xlim=0\n",
    "  plt.ylim=0\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(model, X_test, y_test):\n",
    "  from sklearn.metrics import RocCurveDisplay\n",
    "  tree_ROC = RocCurveDisplay.from_estimator(model, X_test, y_test, color='green', linewidth=3)\n",
    "  plt.title('ROC Curve')\n",
    "  plt.xlabel('False Alarm (1 - Specificity)')\n",
    "  plt.ylabel('Recall (Sensitivity)')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Player_2_winner = 0', 'Player_2_winner = 1'],\n",
    "                yticklabels=['Player_2_winner = 0', 'Player_2_winner = 1'])\n",
    "    plt.xlabel('Predicted Outcome')\n",
    "    plt.ylabel('Actual Outcome')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot variable importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_importance(model, X_train):\n",
    "  importances = pd.Series(data=model.feature_importances_,\n",
    "                          index=X_train.columns)\n",
    "  importances.sort_values().plot(kind='barh', color=\"#00802F\")\n",
    "  plt.title('Features Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test data\n",
    "We will reuse 2  data sets for training each model and then testing (evaluating) the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"winner_player_2\", axis=1)\n",
    "y = train[\"winner_player_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Creation\n",
    "Cross validation and hyperparameter optimisation was done separately - see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)\n",
    "\n",
    "# 1. Instantiate Model\n",
    "model1_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, max_features = 8, min_samples_leaf = 10, min_samples_split = 2, random_state=1)\n",
    "\n",
    "# 2. Fit model\n",
    "model1_tree.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make prediction\n",
    "y_pred = model1_tree.predict(X_test)\n",
    "\n",
    "# 4. Get prediction probabilities\n",
    "model1_tree.predict_proba(X_test)\n",
    "\n",
    "# 5. Evaluate Model Performance - accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "# 6. Print classification report\n",
    "# y_pred =  (model1_tree.predict_proba(X_test)[:, 1] > 0.1).astype(int)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "###########\n",
    "# Run on 19.12\n",
    "# (criterion=\"entropy\", max_depth=10, max_features = 8, min_samples_leaf = 1, min_samples_split = 2, random_state=1)#\n",
    "# Accuracy score: 0.673\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Decision Tree model, with 66.9% there is a slightly better accuracy than our benchmark of 65.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(model1_tree, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation and hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# 1. Define hyperparameters for GridSearchCV\n",
    "parameters = {\n",
    "            'max_depth': [5, 10],\n",
    "            \"max_features\": [4, 8, 16],\n",
    "            'min_samples_leaf': [1, 10, 20, 50],\n",
    "            \"min_samples_split\": [2, 3, 5, 7]\n",
    "            }\n",
    "\n",
    "# 2. Define a scoring function for accuracy\n",
    "acc_score = make_scorer(accuracy_score, greater_is_better=True)\n",
    "\n",
    "# 3. Define GridSearch CV object\n",
    "model1_tree_CV = GridSearchCV(model1_tree, parameters, scoring=acc_score, cv=5,verbose=3) # Apply 5 Cross Validiation Folds to find best hyperparameters\n",
    "\n",
    "# 4. Fit GridSearch CV object to model\n",
    "model1_tree_CV_fitted= model1_tree_CV.fit(X_train, y_train)\n",
    "\n",
    "# 5. Interpret results\n",
    "print(\"Best hyperparameters:\", model1_tree_CV_fitted.best_params_)\n",
    "\n",
    "# 6) Evaluation Generalization Performance\n",
    "y_pred_model1 = model1_tree_CV.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_model1)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "##########\n",
    "# Last run on 18.12\n",
    "# Tested:\n",
    "# parameters = {\n",
    "#             \"max_features\": [4, 8, 16],\n",
    "#             'max_depth': [5, 10],\n",
    "#             \"min_samples_split\": [2, 3, 5, 7], \n",
    "#             'min_samples_leaf': [1, 10, 20, 50]\n",
    "#             }\n",
    "# Results:\n",
    "# Best hyperparameters: {'max_depth': 5, 'max_features': 8, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
    "# Accuracy: 0.673\n",
    "##########\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 2: Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation and cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st attempt (Model 2.1.1) -using GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test at 18.12 15:15 \n",
    "# Test duration 40m\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# 1. Instantiate Model\n",
    "model2_1_1_rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# 2. Define hyperparameters for GridSearchCV\n",
    "parameters = {\n",
    "            'n_estimators': [500, 1000],\n",
    "            # 'max_features': [4, 8, 16],\n",
    "            'max_depth': [5, 10],\n",
    "            'min_samples_split': [3, 5, 7], \n",
    "            # 'min_samples_leaf': [1, 10, 20, 50]\n",
    "            }\n",
    "\n",
    "# 3. Define GridSearchCV object\n",
    "acc_score = make_scorer(accuracy_score, greater_is_better=True)\n",
    "model2_1_1_rf_CV = GridSearchCV(model2_1_1_rf, parameters, cv=5, scoring=acc_score, verbose=3)\n",
    "\n",
    "\n",
    "# 2. Fit GridSearchCV to model data\n",
    "model2_1_1_rf_CV_fitted = model2_1_1_rf_CV.fit(X_train, y_train) # use this for ROC plot\n",
    "\n",
    "# 3. Interpret results\n",
    "print(\"Best hyperparameters:\", model2_1_1_rf_CV_fitted.best_params_)\n",
    "\n",
    "# 4. Get prediction probabilities\n",
    "# y_pred_model2_1_1.predict_proba(X_test)\n",
    "\n",
    "# 5. Evaluate Model Performance - accuracy\n",
    "y_pred_model2_1_1 = model2_1_1_rf_CV.predict(X_test)\n",
    "model2_1_1_acc = accuracy_score(y_test, y_pred_model2_1_1)\n",
    "print('Accuracy: %.3f' % model2_1_1_acc)\n",
    "\n",
    "# 6. Print classification report\n",
    "# y_pred =  (model2_1_1_rf.predict_proba(X_test)[:, 1] > 0.1).astype(int)\n",
    "# print(classification_report(y_test, y_pred_model2_1_1))\n",
    "\n",
    "###########\n",
    "# Tested\n",
    "#  parameters = {\n",
    "            # 'n_estimators': [500, 1000],\n",
    "            # 'max_depth': [5, 10],\n",
    "            # 'min_samples_split': [3, 5, 7]\n",
    "#           }\n",
    "# Results\n",
    "# Best hyperparameters: {'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 1000}\n",
    "# Accuracy: 0.675\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Attempt (Model 2.1.2) - using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test at 18.12 17:00\n",
    "# Test duration 36m\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV \n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# 1. Instantiate Model\n",
    "model2_1_2_rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# 2. Define hyperparameters for RandomizedSearchCV\n",
    "parameters = {\n",
    "            'n_estimators': [500, 700, 1000],\n",
    "            # 'max_features': [4, 8, 16],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            # 'min_samples_split': [2, 3, 5, 7], \n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "\n",
    "# 3. Define RandomizedSearchCV object\n",
    "acc_score = make_scorer(accuracy_score, greater_is_better=True)\n",
    "model2_1_2_rf_CV = RandomizedSearchCV(model2_1_2_rf, parameters, cv=5, scoring=acc_score, verbose=3)\n",
    "\n",
    "\n",
    "# 2. Fit RandomizedSearchCV to model data\n",
    "model2_1_2_rf_CV_fitted = model2_1_2_rf_CV.fit(X_train, y_train) # use this for ROC plot\n",
    "\n",
    "# 3. Interpret results\n",
    "print(\"Best hyperparameters:\", model2_1_2_rf_CV_fitted.best_params_)\n",
    "\n",
    "# 4. Get prediction probabilities\n",
    "# y_pred_model2_1_1.predict_proba(X_test)\n",
    "\n",
    "# 5. Evaluate Model Performance - accuracy\n",
    "y_pred_model2_1_2 = model2_1_2_rf_CV.predict(X_test)\n",
    "model2_1_2_acc = accuracy_score(y_test, y_pred_model2_1_2)\n",
    "print('Accuracy: %.3f' % model2_1_2_acc)\n",
    "\n",
    "# 6. Print classification report\n",
    "# y_pred =  (model2_1_1_rf.predict_proba(X_test)[:, 1] > 0.1).astype(int)\n",
    "# print(classification_report(y_test, y_pred_model2_1_1))\n",
    "\n",
    "###########\n",
    "# Tested\n",
    "#  parameters = {\n",
    "#               'n_estimators': [500, 700, 1000],\n",
    "                # 'max_depth': [5, 10, 20],\n",
    "#               'min_samples_leaf': [1, 2, 4]\n",
    "#                }\n",
    "# Results\n",
    "# Best hyperparameters: {'n_estimators': 1000, 'min_samples_leaf': 2, 'max_depth': 10}\n",
    "# Accuracy: 0.675\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the random state of the test data, as we assume the data are not distributed equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "random_state_value = 1\n",
    "results_random_state_comparison = []\n",
    "for random_state in range(1, 21):\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state_value)\n",
    "\n",
    "  # 1. Instantiate model\n",
    "  model2_rf  = RandomForestClassifier(random_state=1)\n",
    "\n",
    "  # 2. Fit Model to Data\n",
    "  reg = model2_rf.fit(X_train,y_train)\n",
    "\n",
    "  # 3. Make prediction\n",
    "  y_pred = model2_rf.predict(X_test)\n",
    "\n",
    "  # 4. Evaluate Model Performance - accuracy\n",
    "  acc = accuracy_score(y_test, y_pred)\n",
    "  #print('Accuracy: %.3f' % acc)\n",
    "\n",
    "  results_random_state_comparison.append((random_state_value, acc))\n",
    "  random_state_value = random_state_value+1\n",
    "\n",
    "df_random_state_results = pd.DataFrame(results_random_state_comparison, columns=['Random State', 'Accuracy'])\n",
    "print(df_random_state_results)\n",
    "\n",
    "# Result:\n",
    "# Top 2 random_states\n",
    "# randam_state 8 -> Accuracy 0.658\n",
    "# random_state 10, 17, 18, etc. -> Accuracy 0.656\n",
    "\n",
    "# Conclusion: Only random_state 8 will be used from now on as a parameter for the train_test_split() method. All previous models are adapted to \n",
    "#             use random_state 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(model2_1_2_rf_CV_fitted.best_estimator_, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Gradient Booster Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st attempt (Model 3.1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test at 18.12 23:00\n",
    "# Test duration 2h \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# 1. Instantiate Model\n",
    "model3_1_1_gb = GradientBoostingClassifier(random_state=1)\n",
    "\n",
    "# 2. Define hyperparameters for GridSearchCV\n",
    "parameters = {\n",
    "            'n_estimators': [500, 1000],\n",
    "            # 'max_features': [4, 8, 16],\n",
    "            'max_depth': [5, 10],\n",
    "            'min_samples_split': [3, 5, 7], \n",
    "            # 'min_samples_leaf': [1, 10, 20, 50]\n",
    "            }\n",
    "\n",
    "# 3. Define GridSearchCV object\n",
    "acc_score = make_scorer(accuracy_score, greater_is_better=True)\n",
    "model3_1_1_gb_CV = GridSearchCV(model3_1_1_gb, parameters, cv=5, scoring=acc_score, verbose=3)\n",
    "\n",
    "\n",
    "# 2. Fit GridSearchCV to model data\n",
    "model3_1_1_gb_CV_fitted = model3_1_1_gb_CV.fit(X_train, y_train) # use this for ROC plot\n",
    "\n",
    "# 3. Interpret results\n",
    "print(\"Best hyperparameters:\", model3_1_1_gb_CV_fitted.best_params_)\n",
    "\n",
    "# 4. Get prediction probabilities\n",
    "# y_pred_model2_1_1.predict_proba(X_test)\n",
    "\n",
    "# 5. Evaluate Model Performance - accuracy\n",
    "y_pred_model3_1_1 = model3_1_1_gb_CV.predict(X_test)\n",
    "model3_1_1_acc = accuracy_score(y_test, y_pred_model3_1_1)\n",
    "print('Accuracy: %.3f' % model3_1_1_acc)\n",
    "\n",
    "# 6. Print classification report\n",
    "# y_pred =  (model2_1_1_rf.predict_proba(X_test)[:, 1] > 0.1).astype(int)\n",
    "# print(classification_report(y_test, y_pred_model2_1_1))\n",
    "\n",
    "###########\n",
    "# Tested\n",
    "#  parameters = {\n",
    "            # 'n_estimators': [500, 1000],\n",
    "            # 'max_depth': [5, 10],\n",
    "            # 'min_samples_split': [3, 5, 7]\n",
    "#           }\n",
    "# Results\n",
    "# Best hyperparameters: {'max_depth': 5, 'min_samples_split': 7, 'n_estimators': 500}\n",
    "# Accuracy: 0.670\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD VERSION - removed\n",
    "'''\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boostedtrees = GradientBoostingClassifier(max_depth= 2, max_features = 8, min_samples_leaf = 1, min_samples_split = 5, n_estimators = 1000, \n",
    "                                         # learning_rate= 0.5,\n",
    "                                        random_state=1)\n",
    "\n",
    "# Create Train Data\n",
    "X = train.drop(\"winner_player_2\", axis=1)\n",
    "y = train[\"winner_player_2\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "# fit model\n",
    "boostedtrees.fit(X_train, y_train)\n",
    "\n",
    "#make prediction\n",
    "y_pred = boostedtrees.predict(X_test)\n",
    "\n",
    "# get prediction probabilities\n",
    "print(boostedtrees.predict_proba(X_test))\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "###########\n",
    "# Run on 11.12\n",
    "# Accuracy score: 0.6736 \n",
    "# (max_depth= 2, max_features = 8, min_samples_leaf = 1, min_samples_split = 5, n_estimators = 1000, random_state=1)#\n",
    "###########\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd attempt (Model 3.1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test at 18.12 17:00\n",
    "# Test duration 45m\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# 1. Instantiate Model\n",
    "model3_1_2_gb = GradientBoostingClassifier(random_state=1)\n",
    "\n",
    "# 2. Define hyperparameters for RandomizedSearchCV\n",
    "parameters = {\n",
    "            'n_estimators': [500, 700, 1000],\n",
    "            # 'max_features': [4, 8, 16],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            # 'min_samples_split': [2, 3, 5, 7], \n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "\n",
    "# 3. Define RandomizedSearchCV object\n",
    "acc_score = make_scorer(accuracy_score, greater_is_better=True)\n",
    "model3_1_2_gb_CV = RandomizedSearchCV(model3_1_2_gb, parameters, cv=5, scoring=acc_score, verbose=3)\n",
    "\n",
    "\n",
    "# 2. Fit RandomizedSearchCV to model data\n",
    "model3_1_2_gb_CV_fitted = model3_1_2_gb_CV.fit(X_train, y_train) # use this for ROC plot\n",
    "\n",
    "# 3. Interpret results\n",
    "print(\"Best hyperparameters:\", model3_1_2_gb_CV_fitted.best_params_)\n",
    "\n",
    "# 4. Get prediction probabilities\n",
    "# y_pred_model2_1_1.predict_proba(X_test)\n",
    "\n",
    "# 5. Evaluate Model Performance - accuracy\n",
    "y_pred_model3_1_2 = model3_1_2_gb_CV.predict(X_test)\n",
    "model3_1_2_acc = accuracy_score(y_test, y_pred_model3_1_2)\n",
    "print('Accuracy: %.3f' % model3_1_2_acc)\n",
    "\n",
    "# 6. Print classification report\n",
    "# y_pred =  (model2_1_1_rf.predict_proba(X_test)[:, 1] > 0.1).astype(int)\n",
    "# print(classification_report(y_test, y_pred_model2_1_1))\n",
    "\n",
    "###########\n",
    "# Tested\n",
    "#  parameters = {\n",
    "#               'n_estimators': [500, 700, 1000],\n",
    "                # 'max_depth': [5, 10, 20],\n",
    "#               'min_samples_leaf': [1, 2, 4]\n",
    "#                }\n",
    "# Results\n",
    "# Best hyperparameters: {'n_estimators': 500, 'min_samples_leaf': 1, 'max_depth': 5}\n",
    "# Accuracy: 0.669\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3.1.1 ROC Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(model3_1_1_gb_CV, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3.1.2 ROC Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(model3_1_2_gb_CV, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3.1.1 Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(model3_1_1_gb_CV_fitted.best_estimator_, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3.1.2 Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_importance(model3_1_1_gb_CV_fitted.best_estimator_, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search - Gradient Boosted Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VERSIOM - remove\n",
    "'''\n",
    "# use this when looking for the best combination of hyperparamers. \n",
    "# the below example serves the purpose of a cross validation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# hyperparameters for GridSearchCV\n",
    "parameters = {\n",
    "            'max_depth': [2],\n",
    "            \"max_features\": [8],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            \"min_samples_split\": [5], \n",
    "            'n_estimators': [1000, 2000]\n",
    "            }\n",
    "\n",
    "# make a scoring function for accuracy\n",
    "acc_score = make_scorer(accuracy_score, greater_is_better=True)\n",
    "\n",
    "# fit model\n",
    "boosted_model_CV = GridSearchCV(boostedtrees, parameters, scoring=acc_score, cv=5,verbose=3) # Apply 5 Cross Validiation Folds to find best hyperparameters\n",
    "boosted_model_CV.fit(X_train, y_train)\n",
    "\n",
    "###########\n",
    "# CV Run on 18.12 18:30\n",
    "# Runtime xx mins\n",
    "# \n",
    "# {'max_depth': 2,\n",
    "#  'max_features': 8,\n",
    "#  'min_samples_leaf': 10,\n",
    "#  'min_samples_split': 5,\n",
    "#  'n_estimators': 1000}\n",
    "# \n",
    "# ######### \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VERSIOM - remove\n",
    "'''\n",
    "boosted_model_CV.best_params_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion  <a name=\"final-concl\"></a>\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cas-proj",
   "language": "python",
   "name": "cas-proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
